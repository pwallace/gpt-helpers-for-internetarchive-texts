# GPT-supported helper scripts for Internet Archive texts

Python tools for extracting and summarizing content from digitized texts (particularly items in Internet Archive).  

Below is a description of each script and its usage.

---

## 1. GPT-magazine-ToC-finder

**Script:** `magazine-toc-describer.py`

**Purpose:**  
Automates the extraction and transcription of tables of contents (ToC) from digitized magazine issues hosted on the Internet Archive.  
For each issue, the script downloads up to three magazine page preview images, and uses GPT-4o to identify and transcribe the ToC.

**Process:**

- Reads a plaintext file containing one Internet Archive identifier per line.
- For each identifier:
    - Uses the Internet Archive metadata API to list files in the item.
    - Finds the `.jp2.zip` or `.images.zip` file and extracts the base filename.
    - Constructs the preview JPG URL for page `_0001`, then `_0002`, then `_0003` if needed.
    - Downloads the JPG.
    - Sends the JPG to GPT-4o for table of contents extraction.
    - If a valid table of contents is detected, outputs it as a hyphen-bulleted list with the header `TABLE OF CONTENTS` and saves it to `{identifier}_toc.txt`.
    - If no table of contents is detected after all attempts, prints a warning and does not create an output file.

**Usage:**

```bash
python magazine-toc-describer.py identifiers.txt
```

- `identifiers.txt`: Plaintext file with one Internet Archive identifier per line.

**Requirements:**

- OpenAI API key (set as environment variable `OPENAI_API_KEY`)
- Python libraries: `requests`, `Pillow`, `openai`, `base64`

**Typical Workflow:**

1. Prepare a text file listing the Internet Archive identifiers for your magazine issues.
2. Ensure your OpenAI API key is set in your environment.
3. Run the script to extract and transcribe tables of contents.
4. Review the `{identifier}_toc.txt` files for accuracy and completeness.

**Supporting Files:**

- `README-magazine-describer.md`: Additional documentation and tips.
- `SAMPLE-INPUT_middmag.txt`: Example input file (identifiers list).
- `SAMPLE-OUTPUT_middleburyNewspapers_Newsletter_1985_V59N03.txt`: Example output file.

---

## 2. GPT-newspaper-frontpage-describer

**Script:** `newspaper-frontpage-describer.py`

**Purpose:**  
Automates the extraction and summarization of front page articles from digitized newspaper issues hosted on the Internet Archive.  
For each issue, the script downloads the front page preview image and uses GPT-4o to generate a description and summary.

**Process:**

- Reads a plaintext file containing one Internet Archive identifier per line.
- For each identifier:
    - Uses the Internet Archive metadata API to list files in the item.
    - Finds the `.jp2.zip` or `.images.zip` file and extracts the base filename.
    - Constructs the preview JPG URL for the front page.
    - Downloads the JPG.
    - Sends the JPG to GPT-4o for description and summary.
    - Saves the summary to `{identifier}_summary.txt`.

**Usage:**

```bash
python newspaper-frontpage-describer.py identifiers.txt
```

- `identifiers.txt`: Plaintext file with one Internet Archive identifier per line.

**Requirements:**

- OpenAI API key (set as environment variable `OPENAI_API_KEY`)
- Python libraries: `requests`, `Pillow`, `openai`, `base64`

**Typical Workflow:**

1. Prepare a text file listing the Internet Archive identifiers for your newspaper issues.
2. Ensure your OpenAI API key is set in your environment.
3. Run the script to extract and summarize front page content.
4. Review the `{identifier}_summary.txt` files for clarity and completeness.

**Supporting Files:**

- `README-newspaper-frontpage-describer.md`: Additional documentation and tips.
- `SAMPLE-INPUT_campus-issues.txt`: Example input file (identifiers list).
- `SAMPLE-OUTPUT_middleburyNewspapers_2019-05-02.txt`: Example output file.

---

### General Notes

- Both scripts require **Python 3**.
- Input files should be plain text, UTF-8 encoded.
- Output files are written in plain text format.
- For best results, review the README files and comments in each script for additional options or configuration.

---

## 3. GPT-Summarizer: Automated Archival Transcript Summarizer

---

### Overview

**GPT-Summarizer** is a Python script for batch-generating concise, neutral summaries of archival transcript files using OpenAI's GPT models.

This script was written explicitly to use ABBY OCR files generated by Internet Archive during the derivative process for "text" type items; however, it should be able to be used for any plaintext file, or easily modified to be a helper function.

The default prompt is designed for archivists and librarians who need professional, descriptive summaries for cataloging and finding aids.

---

### How It Works

- Reads all `.txt` files from the `source` directory.
- For each transcript:
    - Loads the text and removes line breaks for better processing.
    - Splits the text into chunks based on the model's token limit.
    - Sends each chunk to GPT-3.5-turbo with a system prompt instructing the model to produce a neutral, professional summary.
    - Collects all chunk summaries, combines them, and sends the combined summary back to GPT for final revision.
    - Writes the final revised summary to the `output` directory, prefixed with `gpt_`.

---

### Prompt Design

- The system prompt instructs GPT to avoid value judgments, speculation, and sensitive information (e.g., emails, URLs).
- The summary should be under 200 words, focusing on succinctness, accuracy, readability, and completeness.

---

### Usage

1. Place your transcript `.txt` files in the `source` directory.
2. Set your OpenAI API key as an environment variable:  
   ```bash
   export OPENAI_API_KEY="sk-..."
   ```
3. Install required Python packages:  
   ```bash
   pip install openai tiktoken
   ```
4. Run the script:  
   ```bash
   python gpt-summarizer.py
   ```
5. Summaries will be written to the `output` directory as `gpt_<original_filename>.txt`.

---

### Requirements

- Python 3.7+
- Packages: `openai`, `tiktoken`
- OpenAI API key (set as environment variable `OPENAI_API_KEY`)

---

### Notes

- The script uses GPT-3.5-turbo by default; you may change the model as needed.
- The script is designed for batch processing and will process all `.txt` files in the `source` directory.
- Summaries are revised for clarity and completeness before output.

---

### Example Workflow

1. Prepare your transcript files in the `source` directory.
2. Ensure your OpenAI API key is set.
3. Run the script.
4. Find the generated summaries in the `output` directory.

---

## Troubleshooting

- **API Key Error:** Make sure your OpenAI API key is set as an environment variable.
- **Missing Packages:** Install required packages with `pip install openai tiktoken`.
- **No Output:** Check that your transcript files are in the `source` directory and have the `.txt` extension.
